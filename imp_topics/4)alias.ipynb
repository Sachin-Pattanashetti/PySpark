{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5a6111",
   "metadata": {},
   "source": [
    "# Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is alias in PySpark?\n",
    "\n",
    "# alias is used to give a temporary name to a column or expression in a DataFrame.\n",
    "# Often used with select, withColumn, groupBy, or agg.\n",
    "# It does not rename the column in the original DataFrame; it’s just for the result of that operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, upper, lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"select\").getOrCreate()\n",
    "data = [(\"Alice\", 25, \"USA\"), (\"Bob\", 30, \"UK\")]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"country\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653177c0",
   "metadata": {},
   "source": [
    "3️⃣ Using alias in select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aeebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    col(\"name\").alias(\"employee_name\"),\n",
    "    (col(\"age\") + 5).alias(\"age_plus_5\"),\n",
    "    upper(col(\"country\")).alias(\"country_upper\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f6a84",
   "metadata": {},
   "source": [
    "4️⃣ Using alias in withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e89fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.withColumn(\"age_new\", (col(\"age\") + 10).alias(\"age_plus_10\"))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b44ee3",
   "metadata": {},
   "source": [
    "5️⃣ Using alias in groupBy / agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c062e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"country\") \\\n",
    "  .agg(avg(\"age\").alias(\"average_age\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "638aca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+---------------------------------+---------------------+\n",
      "|Date  |Time       |Focus Area|Topics                           |Interview Angle      |\n",
      "+------+-----------+----------+---------------------------------+---------------------+\n",
      "|26 Dec|10:30-11:00|Warm-up   |Revision + mindset               |Speak answers aloud  |\n",
      "|26 Dec|11:00-13:00|Python    |Lists, Dicts, Comprehensions     |Core Python questions|\n",
      "|26 Dec|13:00-14:00|Break     |Lunch                            |NULL                 |\n",
      "|26 Dec|14:00-15:30|SQL       |Joins, WHERE vs HAVING           |Production SQL       |\n",
      "|26 Dec|15:30-15:45|Break     |Tea                              |NULL                 |\n",
      "|26 Dec|15:45-17:30|PySpark   |RDD vs DataFrame, Lazy evaluation|Internals            |\n",
      "|26 Dec|17:30-18:00|Break     |Rest                             |NULL                 |\n",
      "|26 Dec|18:00-19:30|GCP       |BigQuery basics                  |Analytics design     |\n",
      "|26 Dec|19:30-20:00|Break     |Dinner                           |NULL                 |\n",
      "|26 Dec|20:00-21:30|AWS       |S3 basics & storage classes      |Cost + durability    |\n",
      "|26 Dec|21:30-22:30|Revision  |Mock interview + notes           |Confidence           |\n",
      "|27 Dec|10:30-12:30|Python    |Functions, lambda, map/filter    |ETL usage            |\n",
      "|27 Dec|12:30-13:00|Break     |Rest                             |NULL                 |\n",
      "|27 Dec|13:00-14:30|SQL       |GROUP BY, Aggregates             |Real queries         |\n",
      "|27 Dec|14:30-15:30|Break     |Lunch                            |NULL                 |\n",
      "|27 Dec|15:30-17:00|PySpark   |Transformations vs Actions       |Job execution        |\n",
      "|27 Dec|17:15-18:45|GCP       |BigQuery partition & clustering  |Performance          |\n",
      "|27 Dec|19:30-22:30|AWS       |Athena vs Redshift               |When to use what     |\n",
      "|28 Dec|10:30-12:30|Python    |Generators & memory optimization |Large files          |\n",
      "|28 Dec|13:30-15:00|SQL       |CTE vs Subquery                  |Readability          |\n",
      "+------+-----------+----------+---------------------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59351)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"c:\\Users\\sachi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"ExcelRead\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pdf = pd.read_excel(\n",
    "    r\"C:\\Git files\\My git files\\PySpark\\files\\7_Day_Data_Engineer_Interview_Plan.xlsx\",\n",
    "    sheet_name=\"7-Day Timed Plan\"\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# pivot_df = (\n",
    "#     df.groupBy(\"Time\")          # string OR col() both OK\n",
    "#       .pivot(\"Focus Area\")      # ❗ MUST be string\n",
    "#       .agg(first(\"Topics\"))\n",
    "# )\n",
    "\n",
    "# pivot_df.show(truncate=False)\n",
    "df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
