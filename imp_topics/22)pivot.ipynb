{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "172cae85",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b779d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",   \"month\": \"Jan\", \"sales\": 12000},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",   \"month\": \"Feb\", \"sales\": 15000},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",   \"month\": \"Mar\", \"sales\": 18000},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",  \"month\": \"Jan\", \"sales\": 10000},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",  \"month\": \"Feb\", \"sales\": 17000},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",  \"month\": \"Mar\", \"sales\": 16000},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\",  \"month\": \"Jan\", \"sales\": 9000},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\",  \"month\": \"Feb\", \"sales\": 11000},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\",  \"month\": \"Mar\", \"sales\": 13000},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\",  \"month\": \"Jan\", \"sales\": 14000},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\",  \"month\": \"Feb\", \"sales\": 16000},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\",  \"month\": \"Mar\", \"sales\": 15000},\n",
    "]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(sales_data)\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db97a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+-----+\n",
      "|emp_id| name|  Feb|  Jan|  Mar|\n",
      "+------+-----+-----+-----+-----+\n",
      "|   102|Meena|17000|10000|16000|\n",
      "|   104|Divya|16000|14000|15000|\n",
      "|   101| Arun|15000|12000|18000|\n",
      "|   103|Kiran|11000| 9000|13000|\n",
      "+------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df = (\n",
    "    df.groupBy(\"emp_id\", \"name\")\n",
    "      .pivot(\"month\")\n",
    "      .sum(\"sales\")\n",
    ")\n",
    "\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351bfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_data = [\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",   \"subject\": \"Math\",    \"marks\": 88},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",   \"subject\": \"Science\", \"marks\": 92},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",   \"subject\": \"English\", \"marks\": 85},\n",
    "\n",
    "    {\"student_id\": 2, \"name\": \"Meena\",  \"subject\": \"Math\",    \"marks\": 76},\n",
    "    {\"student_id\": 2, \"name\": \"Meena\",  \"subject\": \"Science\", \"marks\": 81},\n",
    "    {\"student_id\": 2, \"name\": \"Meena\",  \"subject\": \"English\", \"marks\": 78},\n",
    "\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\",  \"subject\": \"Math\",    \"marks\": 90},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\",  \"subject\": \"Science\", \"marks\": 88},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\",  \"subject\": \"English\", \"marks\": 84},\n",
    "\n",
    "    {\"student_id\": 4, \"name\": \"Divya\",  \"subject\": \"Math\",    \"marks\": 69},\n",
    "    {\"student_id\": 4, \"name\": \"Divya\",  \"subject\": \"Science\", \"marks\": 73},\n",
    "    {\"student_id\": 4, \"name\": \"Divya\",  \"subject\": \"English\", \"marks\": 80},\n",
    "]\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_s = spark.createDataFrame(students_data)\n",
    "# df_s.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396a6eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+-------+-----------+----------+\n",
      "|student_id| name|English|Math|Science|total_marks|Percentage|\n",
      "+----------+-----+-------+----+-------+-----------+----------+\n",
      "|         1| Arun|     85|  88|     92|        265|    88.33%|\n",
      "|         3|Kiran|     84|  90|     88|        262|    87.33%|\n",
      "|         2|Meena|     78|  76|     81|        235|    78.33%|\n",
      "|         4|Divya|     80|  69|     73|        222|     74.0%|\n",
      "+----------+-----+-------+----+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1. Pivot the data to show each student's marks in Math, Science, and English in separate columns.\n",
    "from pyspark.sql.functions import col,round,concat,lit\n",
    " \n",
    "df_s = df_s.groupBy(\"student_id\",\"name\")\\\n",
    "    .pivot(\"subject\")\\\n",
    "    .sum(\"marks\")\\\n",
    "    .withColumn(\"total_marks\",(col(\"English\")+col(\"Math\")+col(\"Science\")))\\\n",
    "    .withColumn(\"Percentage\", concat(round((col(\"total_marks\")/300) * 100, 2),lit(\"%\")))\\\n",
    "    .orderBy(col(\"total_marks\").desc())\n",
    "\n",
    "df_s.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
