{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from  pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ui\").getOrCreate()\n",
    "\n",
    "employees = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",    \"dept\": \"IT\",      \"salary\": 80000, \"experience\": 5,  \"skills\": [\"Python\", \"SQL\"],      \"status\": \"Active\",   \"hire_date\": \"2021-01-10\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",   \"dept\": \"HR\",      \"salary\": 55000, \"experience\": 3,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2022-03-15\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\",   \"dept\": \"IT\",      \"salary\": 90000, \"experience\": 7,  \"skills\": [\"Python\", \"PySpark\"],  \"status\": \"Resigned\", \"hire_date\": \"2018-10-01\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Bhavani\", \"dept\": \"Finance\", \"salary\": 65000, \"experience\": 4,  \"skills\": [\"Excel\", \"SQL\"],        \"status\": \"Active\",   \"hire_date\": \"2020-07-19\"},\n",
    "    {\"emp_id\": 105, \"name\": \"Akash\",   \"dept\": \"IT\",      \"salary\": 75000, \"experience\": 2,  \"skills\": [\"Python\"],              \"status\": \"Active\",   \"hire_date\": \"2023-05-02\"},\n",
    "    \n",
    "    {\"emp_id\": 106, \"name\": \"Ravi\",    \"dept\": \"Sales\",   \"salary\": 45000, \"experience\": 1,  \"skills\": [],                      \"status\": \"Active\",   \"hire_date\": \"2024-01-11\"},\n",
    "    {\"emp_id\": 107, \"name\": \"Divya\",   \"dept\": \"HR\",      \"salary\": None,  \"experience\": 6,  \"skills\": [\"Excel\", \"PowerBI\"],    \"status\": \"Active\",   \"hire_date\": \"2019-11-28\"},\n",
    "    {\"emp_id\": 108, \"name\": \"Suresh\",  \"dept\": \"Sales\",   \"salary\": 47000, \"experience\": 2,  \"skills\": [\"Negotiation\"],         \"status\": \"Active\",   \"hire_date\": \"2021-06-22\"},\n",
    "    {\"emp_id\": 109, \"name\": \"Jaya\",    \"dept\": \"IT\",      \"salary\": 80000, \"experience\": 5,  \"skills\": [\"SQL\"],                 \"status\": \"Active\",   \"hire_date\": \"2020-09-10\"},\n",
    "    {\"emp_id\": 110, \"name\": \"Vijay\",   \"dept\": \"Finance\", \"salary\": 70000, \"experience\": 8,  \"skills\": [\"Excel\"],               \"status\": \"Resigned\", \"hire_date\": \"2017-12-01\"},\n",
    "\n",
    "    {\"emp_id\": 111, \"name\": \"Rohit\",   \"dept\": \"IT\",      \"salary\": 65000, \"experience\": 3,  \"skills\": [\"Python\"],              \"status\": \"Active\",   \"hire_date\": \"2022-02-17\"},\n",
    "    {\"emp_id\": 112, \"name\": \"Maya\",    \"dept\": \"HR\",      \"salary\": 62000, \"experience\": 4,  \"skills\": None,                    \"status\": \"Active\",   \"hire_date\": None},\n",
    "    {\"emp_id\": 113, \"name\": \"Goutham\", \"dept\": \"Sales\",   \"salary\": 52000, \"experience\": 3,  \"skills\": [\"Negotiation\", \"CRM\"],  \"status\": \"Active\",   \"hire_date\": \"2021-08-05\"},\n",
    "    {\"emp_id\": 114, \"name\": \"Lavanya\", \"dept\": \"IT\",      \"salary\": 90000, \"experience\": 10, \"skills\": [\"Python\", \"AWS\"],      \"status\": \"Active\",   \"hire_date\": \"2015-07-14\"},\n",
    "    {\"emp_id\": 115, \"name\": \"Kavya\",   \"dept\": \"Finance\", \"salary\": 58000, \"experience\": 1,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2023-01-20\"},\n",
    "\n",
    "    {\"emp_id\": 116, \"name\": \"Manish\",  \"dept\": \"IT\",      \"salary\": None,  \"experience\": 4,  \"skills\": [\"SQL\"],                 \"status\": \"Active\",   \"hire_date\": \"2020-10-05\"},\n",
    "    {\"emp_id\": 117, \"name\": \"Rakesh\",  \"dept\": \"Sales\",   \"salary\": 48000, \"experience\": 2,  \"skills\": [\"CRM\"],                 \"status\": None,       \"hire_date\": \"2022-11-11\"},\n",
    "    {\"emp_id\": 118, \"name\": \"Anita\",   \"dept\": \"HR\",      \"salary\": 56000, \"experience\": 3,  \"skills\": [\"Excel\"],               \"status\": \"Active\",   \"hire_date\": \"2023-03-01\"},\n",
    "    {\"emp_id\": 119, \"name\": \"Tarun\",   \"dept\": \"Finance\", \"salary\": 70000, \"experience\": 7,  \"skills\": [\"SQL\", \"PowerBI\"],      \"status\": \"Active\",   \"hire_date\": \"2018-02-25\"},\n",
    "    {\"emp_id\": 120, \"name\": \"Sanjay\",  \"dept\": \"IT\",      \"salary\": 76000, \"experience\": 5,  \"skills\": [\"Python\", \"SQL\"],       \"status\": \"Active\",   \"hire_date\": \"2019-09-09\"}\n",
    "]\n",
    "\n",
    "print(\"Spark UI URL:\", spark.sparkContext.uiWebUrl)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"practice\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Question 1\n",
    "# Write PySpark code to filter only employees from the IT department whose salary is greater than 75,000.\n",
    "print(\"Spark UI URL:\", spark.sparkContext.uiWebUrl)\n",
    "df.filter((F.col(\"dept\")=='IT') & (F.col(\"salary\") >75000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a009eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:\n",
    "# Write a PySpark query to find all employees who joined in or after the year 2021.\n",
    "\n",
    "df.filter(F.year(F.col(\"hire_date\"))>=2021).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find employees who have Python in their skills.\n",
    "df.filter(F.array_contains(\"skills\",\"Python\")).select('name','skills').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest-paid employee in each department using a window function.\n",
    "# Return: emp_id, name, dept, salary, rank.\n",
    "print(\"Spark UI URL:\", spark.sparkContext.uiWebUrl)\n",
    "win = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df1 = df.withColumn(\"rank\",F.rank().over(win))\\\n",
    "  .withColumn(\"dense_rank\",F.dense_rank().over(win))\\\n",
    "  .filter(F.col(\"rank\")==1).select(\"emp_id\",\"name\",\"dept\",\"salary\",\"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to explode the skills array so that each skill appears in a separate row.\n",
    "# Return only:\n",
    "# emp_id, name, skill\n",
    "\n",
    "df2 = df.withColumn(\"skill\",F.explode(\"skills\"))\n",
    "df2.select('emp_id','name','skill').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1124a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to replace all null salaries with 0.\n",
    "df.fillna(0,subset=[\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a PySpark query to count how many employees are present in each department.\n",
    "# Return columns: dept, emp_count\n",
    "\n",
    "df3 = df.groupBy(\"dept\").agg(F.count(\"name\").alias(\"number_of_emp\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some employees have status = null.\n",
    "# Write a PySpark query to replace null status with \"Unknown\" using when and otherwise.\n",
    "\n",
    "df4 =df.withColumn(\"status\",F.when(F.col(\"status\").isNull(),\"Unknown\").otherwise(F.col(\"status\")))\n",
    "df4.filter(F.col(\"status\")=='Unknown').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ed511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average salary of each department, ignoring null salaries.\n",
    "# Return: dept, avg_salary\n",
    "\n",
    "df5 = df.filter(F.col(\"salary\").isNotNull())\n",
    "df5 =df5.groupBy(\"dept\").agg(F.mean(\"salary\").alias(\"avg_salary\"))\n",
    "df5.select('dept','avg_salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50468360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find the second highest salary in each department.\n",
    "# Return: dept, emp_id, name, salary.\n",
    "\n",
    "wind = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df6 = df.withColumn(\"rank\",F.rank().over(wind))\n",
    "df6.filter(F.col(\"rank\")==2).select('dept','emp_id','name','salary').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.withColumn(\"skill\",F.explode(\"skills\"))\n",
    "unique_skills_count = df7.select(\"skill\").distinct().count()  # count distinct skills\n",
    "print(unique_skills_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group employees by department and collect all employee names into a list for each department.\n",
    "# Return columns: dept, emp_list.\n",
    "df.groupBy(\"dept\").agg(F.collect_list('name')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea63931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_fixed = df.withColumn(\n",
    "    \"skills\",\n",
    "    F.when(F.col(\"skills\").isNull(), F.array()).otherwise(F.col(\"skills\"))\n",
    ")\n",
    "\n",
    "df_exploded = df_fixed.withColumn(\"skill\", F.explode(\"skills\"))\n",
    "\n",
    "df_exploded.select(\"emp_id\", \"name\", \"skill\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('hire_date',F.to_date(F.col(\"hire_date\"),'yyyy-MM-dd'))\n",
    "\n",
    "df = df.withColumn(\"days_with_company\",F.date_diff(F.current_date(),'hire_date'))\n",
    "\n",
    "df.select('emp_id', 'name', 'hire_date', 'days_with_company').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find employees whose name starts with 'A' or ends with 'y'.\n",
    "# Return: emp_id, name, dept.\n",
    "\n",
    "df.filter((F.col('name').startswith('A')) | (F.col('name').endswith('y')))\\\n",
    "  .select('emp_id','name','dept').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find all duplicate employee names and the count of how many times each name appears.\n",
    "# Return columns: name, count.\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"name\") \\\n",
    "  .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "  .filter(F.col(\"count\") > 1) \\\n",
    "  .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort employees first by department ascending, then salary descending.\n",
    "# Return columns: emp_id, name, dept, salary.\n",
    "\n",
    "df1 =df.sort((['dept','salary']),ascending=[1,0])\n",
    "df1.select('emp_id','name','dept','salary').show(5)\n",
    "\n",
    "df1 = df.orderBy(F.col(\"dept\").asc(), F.col(\"salary\").desc())\n",
    "df1.select('emp_id','name','dept','salary').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb2cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 2 highest-paid employees in each department.\n",
    "# Return columns: dept, emp_id, name, salary.\n",
    "\n",
    "\n",
    "w = Window.partitionBy(\"dept\").orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df.withColumn('rank',F.rank().over(w))\\\n",
    "  .filter(F.col(\"rank\")<=2).select(\"dept\", \"emp_id\", \"name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null salaries with the average salary of their department.\n",
    "# Return columns: emp_id, name, dept, salary.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window partitioned by department\n",
    "win = Window.partitionBy(\"dept\")\n",
    "\n",
    "# Replace null salaries with average salary of the department\n",
    "df_filled = df.withColumn(\n",
    "    \"salary\",\n",
    "    F.when(\n",
    "        F.col(\"salary\").isNull(),\n",
    "        F.round(F.avg(\"salary\").over(win), 2)  # round to 2 decimals (optional)\n",
    "    ).otherwise(F.col(\"salary\"))\n",
    ")\n",
    "\n",
    "# df_filled.select(\"emp_id\", \"name\", \"dept\", \"salary\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window to get dept-wise average salary\n",
    "win = Window.partitionBy(\"dept\")\n",
    "\n",
    "df_filled = df.withColumn(\n",
    "    \"salary\",\n",
    "    F.coalesce(F.col(\"salary\"), F.round(F.avg(\"salary\").over(win),2))\n",
    ")\n",
    "\n",
    "df_filled.select(\"emp_id\", \"name\", \"dept\", \"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('skill',F.explode('skills'))\n",
    "df.groupBy('skill').agg(F.count('name').alias('emp_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sales_data = [\n",
    "    {\"txn_id\": 1, \"cust_id\": 101, \"amount\": 500,  \"status\": \"success\", \"txn_date\": \"2024-01-01\"},\n",
    "    {\"txn_id\": 2, \"cust_id\": 102, \"amount\": None, \"status\": \"success\", \"txn_date\": \"2024-01-01\"},\n",
    "    {\"txn_id\": 3, \"cust_id\": 101, \"amount\": 200,  \"status\": \"failed\",  \"txn_date\": \"2024-01-03\"},\n",
    "    {\"txn_id\": 4, \"cust_id\": 103, \"amount\": 800,  \"status\": \"success\", \"txn_date\": \"2024-01-04\"},\n",
    "    {\"txn_id\": 5, \"cust_id\": 104, \"amount\": None, \"status\": \"failed\",  \"txn_date\": \"2024-01-05\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Replace null amount with 0\n",
    "df = df.fillna(0,subset=['amount'])\n",
    "\n",
    "# Convert txn_date into proper date type\n",
    "df = df.withColumn('txn_date',to_date(col('txn_date'),'yyyy-MM-dd'))\n",
    "# df = df.withColumn('hire_date',F.to_date(F.col(\"hire_date\"),'yyyy-MM-dd'))\n",
    "\n",
    "# Add a new column is_success →\n",
    "# 1 if status = \"success\"\n",
    "#  ->0 otherwise\n",
    "df = df.withColumn(\"is_success\",when(col('status')=='success',1).otherwise(0))\n",
    "\n",
    "# Filter only transactions where amount > 0\n",
    "df = df.filter(col('amount')>0)\n",
    "\n",
    "# Final output columns:\n",
    "# txn_id, cust_id, amount, txn_date, is_success\n",
    "df.select('txn_id', 'cust_id', 'amount', 'txn_date', 'is_success').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92291926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "order_data = [\n",
    "    {\"order_id\": 1, \"cust_id\": 101, \"amount\": 500, \"category\": \"Electronics\", \"order_date\": \"2024-01-01\"},\n",
    "    {\"order_id\": 2, \"cust_id\": 101, \"amount\": 300, \"category\": \"Grocery\",     \"order_date\": \"2024-01-03\"},\n",
    "    {\"order_id\": 3, \"cust_id\": 102, \"amount\": 900, \"category\": \"Electronics\", \"order_date\": \"2024-01-05\"},\n",
    "    {\"order_id\": 4, \"cust_id\": 103, \"amount\": None,\"category\": \"Grocery\",     \"order_date\": \"2024-01-06\"},\n",
    "    {\"order_id\": 5, \"cust_id\": 103, \"amount\": 600, \"category\": \"Fashion\",     \"order_date\": \"2024-01-07\"},\n",
    "    {\"order_id\": 6, \"cust_id\": 101, \"amount\": 200, \"category\": \"Fashion\",     \"order_date\": \"2024-01-09\"}\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"interview\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(order_data)\n",
    "\n",
    "# 1️⃣ Replace null amount with the average amount of that category\n",
    "\n",
    "win = Window.partitionBy('category')\n",
    "df = df.withColumn(\"amount\",coalesce('amount',avg('amount').over(win)))\n",
    "df.select('order_id', 'cust_id', 'amount', 'category', 'order_date').show()\n",
    "\n",
    "\n",
    "# 2️⃣ Find total amount per customer\n",
    "df.groupBy('cust_id').agg(sum('amount').alias(\"total amount\")).show()\n",
    "\n",
    "# 3️⃣ For each category, find the highest amount order\n",
    "win = Window.partitionBy('category').orderBy(col('amount').desc())\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(win))\\\n",
    "  .filter(col(\"dense_rank\")==1)\\\n",
    "  .select('category',\"amount\").show()\n",
    "\n",
    "# 4️⃣ Add a new column days_since_order using current_date\n",
    "df.withColumn(\"days_since_order\",current_date()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac77ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Example - PySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"spark-ui:,{spark.sparkContext.uiWebUrl}\")\n",
    "# Create first DataFrame: Employees\n",
    "employees = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"John\", 1000),\n",
    "        (2, \"Alice\", 1200),\n",
    "        (3, \"Bob\", 1500)\n",
    "    ],\n",
    "    [\"emp_id\", \"name\", \"salary\"]\n",
    ")\n",
    "\n",
    "# Create second DataFrame: Departments\n",
    "departments = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"HR\"),\n",
    "        (2, \"Engineering\"),\n",
    "        (4, \"Marketing\")\n",
    "    ],\n",
    "    [\"emp_id\", \"department\"]\n",
    ")\n",
    "\n",
    "# Perform inner join on emp_id\n",
    "joined_df = employees.join(departments, on=\"emp_id\", how=\"inner\")\n",
    "\n",
    "# Show result\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46496c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KPMG\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreateDF\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", \"Badminton, Tennis\"),\n",
    "    (\"Bob\", \"Tennis, Cricket\"),\n",
    "    (\"Julie\", \"Cricket, Carroms\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"hobbies\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df1 = df.withColumn(\"Hobbies\",explode(split(col(\"hobbies\"),',')))\n",
    "df1.show()\n",
    "\n",
    "df2 = df1.groupBy('name').agg(collect_list('Hobbies').alias(\"new_hobbies\"))\n",
    "df2.withColumn(\n",
    "    \"new_hobbies_str\",\n",
    "    concat(lit('\"'),concat_ws(\", \", \"new_hobbies\"),lit('\"'))\n",
    ").show(truncate=False)\n",
    "# df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce, when\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Step 1: Create DataFrame\n",
    "data = [\n",
    "    (\"Goa\", \"\", \"AP\"),\n",
    "    (\"\", \"AP\", None),\n",
    "    (None, \"\", \"bglr\")\n",
    "]\n",
    "\n",
    "columns = [\"city1\", \"city2\", \"city3\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Original DataFrame\")\n",
    "df.show()\n",
    "\n",
    "# Step 2: Create cities column\n",
    "df = df.withColumn(\n",
    "    \"cities\",\n",
    "    coalesce(\n",
    "        when(col(\"city1\") != \"\", col(\"city1\")),\n",
    "        when(col(\"city2\") != \"\", col(\"city2\")),\n",
    "        when(col(\"city3\") != \"\", col(\"city3\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Final DataFrame\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Student table\n",
    "student_data = [\n",
    "    (1, \"Steve\"),\n",
    "    (2, \"David\"),\n",
    "    (3, \"Aryan\")\n",
    "]\n",
    "\n",
    "student_columns = [\"student_id\", \"student_name\"]\n",
    "\n",
    "df_students = spark.createDataFrame(student_data, student_columns)\n",
    "\n",
    "# Marks table\n",
    "marks_data = [\n",
    "    (1, \"pyspark\", 90),\n",
    "    (1, \"sql\", 100),\n",
    "    (2, \"sql\", 70),\n",
    "    (2, \"pyspark\", 60),\n",
    "    (3, \"sql\", 30),\n",
    "    (3, \"pyspark\", 20)\n",
    "]\n",
    "\n",
    "marks_columns = [\"student_id\", \"subject_name\", \"marks\"]\n",
    "\n",
    "df_marks = spark.createDataFrame(marks_data, marks_columns)\n",
    "\n",
    "df_marks = df_marks.groupBy('student_id').agg(((sum('marks')/200)*100).alias('marks'))\n",
    "\n",
    "df_join = df_students.join(df_marks,how='left',on='student_id')\n",
    "df_join=df_join.withColumn('marks',col('marks').cast('int'))\n",
    "df_join.withColumn(\"Result\",when(col('marks')>=70,'Distinction')\\\n",
    "                            .when(col('marks').between(60,69),'first class')\\\n",
    "                            .when(col('marks').between(50,59),'second class')\\\n",
    "                            .when(col('marks').between(40,49),'third class')\n",
    "                            .otherwise('Fail')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC Write a solution to find the percentage of immediate orders in the first order of all customers, rounded to 2 decimal places.\n",
    "# MAGIC --------------------\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "# MAGIC\n",
    "# MAGIC The first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "print(f\"spark url :,{spark.sparkContext.uiWebUrl}\")\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"delivery_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_pref_delivery_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, 1, \"2019-08-01\", \"2019-08-02\"),\n",
    "    (2, 2, \"2019-08-02\", \"2019-08-02\"),\n",
    "    (3, 1, \"2019-08-11\", \"2019-08-12\"),\n",
    "    (4, 3, \"2019-08-24\", \"2019-08-24\"),\n",
    "    (5, 3, \"2019-08-21\", \"2019-08-22\"),\n",
    "    (6, 2, \"2019-08-11\", \"2019-08-13\"),\n",
    "    (7, 4, \"2019-08-09\", \"2019-08-09\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Find the first order for all the customers\n",
    "first_order = df.groupBy(col(\"customer_id\")).agg(min(col(\"order_date\")).alias(\"first_order_date\"))\n",
    "first_order.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Join the first_order date with the original customer order data\n",
    "first_order_joined = df.join(first_order, df[\"customer_id\"]==first_order[\"customer_id\"], \"inner\").select(df[\"*\"], first_order[\"first_order_date\"])\n",
    "first_order_joined.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "# Filter immediate deliveries\n",
    "immediate_deliveries = first_order_joined.filter(col(\"first_order_date\") == col(\"customer_pref_delivery_date\"))\n",
    "immediate_deliveries.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate total orders and immediate orders\n",
    "total_orders = df.count()\n",
    "immediate_orders = immediate_deliveries.count()\n",
    "print(\"immediate_orders: \", immediate_orders, \", total_orders: \", total_orders)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate percentage\n",
    "percentage = (immediate_orders / total_orders) * 100 if total_orders > 0 else 0\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of immediate orders: {percentage:.2f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Using DF API\n",
    "# MAGIC ----------------\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate immediate deliveries\n",
    "df_with_immediate = first_order_joined\\\n",
    "    .withColumn(\"is_immediate\", when(col(\"first_order_date\") == col(\"customer_pref_delivery_date\"), 1).otherwise(0))\n",
    "\n",
    "# Aggregate to find the percentage of immediate deliveries\n",
    "result_df = df_with_immediate.agg(round((sum(col(\"is_immediate\")) / count(lit(1)) * 100), 2).alias(\"immediate_order_percentage\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe688e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HybridEmployeeData\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"week_day\", StringType(), True),\n",
    "    StructField(\"first_login\", StringType(), True),\n",
    "    StructField(\"last_logout\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    # Week 1\n",
    "    (101, \"Akash\", \"2024-01-01\", \"Monday\",    \"09:05\", \"18:10\"),\n",
    "    (101, \"Akash\", \"2024-01-02\", \"Tuesday\",   \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-03\", \"Wednesday\", \"09:10\", \"18:20\"),\n",
    "    (101, \"Akash\", \"2024-01-04\", \"Thursday\",  \"09:00\", \"18:05\"),\n",
    "    (101, \"Akash\", \"2024-01-05\", \"Friday\",    \"09:15\", \"17:50\"),\n",
    "\n",
    "    # Week 2\n",
    "    (101, \"Akash\", \"2024-01-08\", \"Monday\",    \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-09\", \"Tuesday\",   \"09:05\", \"18:10\"),\n",
    "    (101, \"Akash\", \"2024-01-10\", \"Wednesday\", \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-11\", \"Thursday\",  \"09:20\", \"18:30\"),\n",
    "    (101, \"Akash\", \"2024-01-12\", \"Friday\",    \"09:10\", \"17:45\"),\n",
    "\n",
    "    # Week 3\n",
    "    (101, \"Akash\", \"2024-01-15\", \"Monday\",    \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-16\", \"Tuesday\",   \"09:10\", \"18:15\"),\n",
    "    (101, \"Akash\", \"2024-01-17\", \"Wednesday\", \"09:05\", \"18:05\"),\n",
    "    (101, \"Akash\", \"2024-01-18\", \"Thursday\",  \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-19\", \"Friday\",    \"09:20\", \"17:55\"),\n",
    "\n",
    "    # Week 4\n",
    "    (101, \"Akash\", \"2024-01-22\", \"Monday\",    \"09:05\", \"18:10\"),\n",
    "    (101, \"Akash\", \"2024-01-23\", \"Tuesday\",   \"09:00\", \"18:00\"),\n",
    "    (101, \"Akash\", \"2024-01-24\", \"Wednesday\", \"09:15\", \"18:25\"),\n",
    "    (101, \"Akash\", \"2024-01-25\", \"Thursday\",  \"09:00\", \"18:05\"),\n",
    "    (101, \"Akash\", \"2024-01-26\", \"Friday\",    \"09:10\", \"17:50\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "# df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('first_login',date_format(col(\"first_login\"), \"HH:mm:ss\"))\\\n",
    "       .withColumn('last_logout',date_format(col(\"first_login\"), \"HH:mm:ss\"))\\\n",
    "       .withColumn('date',to_date(col(\"date\")))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06becfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark url :http://192.168.1.9:4040\n",
      "Customers who never made an order in last 3 months:\n",
      "+-----------+-------------+\n",
      "|customer_id|customer_name|\n",
      "+-----------+-------------+\n",
      "|          3|      Charlie|\n",
      "|          4|        David|\n",
      "+-----------+-------------+\n",
      "\n",
      "All customers with number of orders in last 3 months:\n",
      "+-----------+-------------+--------------------+\n",
      "|customer_id|customer_name|orders_last_3_months|\n",
      "+-----------+-------------+--------------------+\n",
      "|          1|        Alice|                   2|\n",
      "|          2|          Bob|                   1|\n",
      "|          3|      Charlie|                   0|\n",
      "|          4|        David|                   0|\n",
      "+-----------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum as _sum, current_date, date_sub\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerOrders\").getOrCreate()\n",
    "print(f\"spark url :{spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Sample customer data\n",
    "customer_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "# Sample orders data\n",
    "orders_data = [\n",
    "    (101, 1, \"2025-10-10\", 500),\n",
    "    (102, 1, \"2025-11-05\", 200),\n",
    "    (103, 2, \"2025-09-15\", 300),\n",
    "    (104, 2, \"2025-11-20\", 150),\n",
    "    (105, 3, \"2025-06-01\", 400)  # older than 3 months\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "orders_df = orders_df.withColumn(\"order_date\",to_date(col(\"order_date\")))\n",
    "\n",
    "# Define the date 3 months ago\n",
    "three_months_ago = date_sub(current_date(), 90)\n",
    "\n",
    "# Orders in last 3 months\n",
    "recent_orders_df = orders_df.filter(col(\"order_date\") >= three_months_ago)\n",
    "\n",
    "# Count orders per customer in last 3 months\n",
    "orders_count_df = recent_orders_df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"orders_last_3_months\"))\n",
    "\n",
    "# Customers who never made an order in last 3 months\n",
    "customers_no_orders_df = customers_df.join(orders_count_df, \"customer_id\", \"left\") \\\n",
    "                                     .filter(col(\"orders_last_3_months\").isNull()) \\\n",
    "                                     .select(\"customer_id\", \"customer_name\")\n",
    "\n",
    "# Customers with orders in last 3 months\n",
    "customers_with_orders_df = customers_df.join(orders_count_df, \"customer_id\", \"left\") \\\n",
    "                                       .na.fill(0)  # Fill 0 for customers with no recent orders\n",
    "\n",
    "# Show results\n",
    "print(\"Customers who never made an order in last 3 months:\")\n",
    "customers_no_orders_df.show()\n",
    "\n",
    "print(\"All customers with number of orders in last 3 months:\")\n",
    "customers_with_orders_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
